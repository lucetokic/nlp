{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LucijaTokic\\Documents\\AA\\01 - Projects\\01 - NLP\\.paraphrasing\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "from nearpy import Engine\n",
    "from nearpy.hashes import RandomBinaryProjections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train = pd.DataFrame(load_dataset(\"tapaco\", \"en\", split='train')) #[0:157000]\n",
    "#test = load_dataset(\"tapaco\", \"en\", split='train[157000:]')\n",
    "n = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = train[['paraphrase_set_id', 'paraphrase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  paraphrase_set_id               paraphrase materials  number\n",
      "0                 1        I ate the cheese.   plastic     703\n",
      "1                 1            I eat cheese.   leather     985\n",
      "2                 1     I'm eating a yogurt.     metal      86\n",
      "3                 1       I'm eating cheese.  aluminum     307\n",
      "4                 1  I'm having some cheese.  aluminum     393\n",
      "5                 1       I eat some cheese.    bamboo     740\n",
      "6                 1       I ate some cheese.      wood      93\n",
      "7                 5             It's Monday.  aluminum     776\n",
      "8                 5      It is Monday today.     stone     841\n",
      "9                 5       It's Monday today.     metal     135\n"
     ]
    }
   ],
   "source": [
    "# List of materials\n",
    "materials = pd.DataFrame({'materials': [\"glass\", \"metal\", \"wood\", \"plastic\", \"paper\", \"fabric\", \"stone\", \"ceramic\",\n",
    "                                        \"rubber\", \"leather\", \"concrete\", \"diamond\", \"silk\", \"aluminum\", \"copper\",\n",
    "                                        \"bronze\", \"silver\", \"gold\", \"bamboo\"]})\n",
    "materials_sampled = materials.sample(n, replace=True, random_state=42)\n",
    "materials_sampled = materials_sampled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "random_numbers = pd.DataFrame({\"number\": [rd.randint(1, 1000) for _ in range(n)]})\n",
    "\n",
    "X_train_raw = pd.concat([X_train_raw, materials_sampled, random_numbers], axis=1)\n",
    "print(X_train_raw.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge columns into one\n",
    "X_train = pd.DataFrame({\"paraphrase_set_id\": X_train_raw.paraphrase_set_id, \"paraphrase_all\": X_train_raw.paraphrase})\n",
    "X_train['paraphrase_all'] = X_train_raw[X_train_raw.columns[1:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to xlsx\n",
    "#with pd.ExcelWriter(\"C:/Users/LucijaTokic/OneDrive - Clear Peaks SL/Documentos/02 - Projects/06 - ADNOC/01-NLP\"\n",
    "#                    \"/tapaco_expanded.xlsx\") as writer:\n",
    "#    X_train_raw.to_excel(writer, sheet_name=\"raw_dataset\", float_format=\"%.2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing only with paraphrasing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tapaco\", \"en\", split='train[0:1000]')\n",
    "X_train_raw = dataset['paraphrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158053, 449)\n"
     ]
    }
   ],
   "source": [
    "# embedding\n",
    "tv = TfidfVectorizer(binary=False, norm=None, use_idf=False, smooth_idf=False, lowercase=True, stop_words='english',\n",
    "                     min_df=0.001, max_df=0.9, max_features=None, ngram_range=(1,1))\n",
    "X_train_emb = pd.DataFrame(tv.fit_transform(X_train.paraphrase_all).toarray())\n",
    "print(X_train_emb.shape)\n",
    "#print(X_train_emb.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paraphrase-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = model.encode(X_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = pd.DataFrame(X_train_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LSH\n",
    "# convert it to numpy array\n",
    "embeddings_array = X_train_emb.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of our vector space\n",
    "dimension = X_train_emb.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First way: RandomBinaryProjections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random binary hash with 10 bits\n",
    "rbp = RandomBinaryProjections('rbp', 20)\n",
    "\n",
    "# Create engine with pipeline configuration\n",
    "engine = Engine(dimension, lshashes=[rbp])\n",
    "\n",
    "# Add the embeddings to the LSH engine\n",
    "for idx, embedding in enumerate(embeddings_array):\n",
    "    engine.store_vector(embedding, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "                 ...              \n",
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "0    I ate the cheese.,plastic,703\n",
      "Name: paraphrase_all, Length: 934, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create random query vector\n",
    "neighbors = engine.neighbours(embeddings_array[10])\n",
    "#print(neighbors)\n",
    "\n",
    "# Get the index of the nearest neighbor\n",
    "nearest_neighbor_index = neighbors[0][0]\n",
    "#print(nearest_neighbor_index)\n",
    "#print(type(nearest_neighbor_index))\n",
    "\n",
    "# Retrieve the corresponding text from X_train_emb\n",
    "nearest_neighbor_text = X_train.iloc[nearest_neighbor_index]['paraphrase_all']\n",
    "\n",
    "# Print the nearest neighbor's text\n",
    "print(nearest_neighbor_text)\n",
    "#print(nearest_neighbor_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second way: Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbits == 2\n",
      "158053 / 4 = 39513.25\n",
      "nbits == 4\n",
      "158053 / 16 = 9878.3125\n",
      "nbits == 8\n",
      "158053 / 256 = 617.39453125\n",
      "nbits == 16\n",
      "158053 / 65536 = 2.4116973876953125\n",
      "nbits == 24\n",
      "158053 / 16777216 = 0.009420692920684814\n",
      "nbits == 32\n",
      "158053 / 4294967296 = 3.6799581721425056e-05\n"
     ]
    }
   ],
   "source": [
    "for nbits in [2, 4, 8, 16, 24, 32]:\n",
    "    buckets = 1 << nbits\n",
    "    print(f\"nbits == {nbits}\")\n",
    "    print(f\"{X_train_emb.shape[0]} / {buckets} = {X_train_emb.shape[0]/buckets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the index using our vectors dimensionality (128) and nbits\n",
    "nbits = 8\n",
    "index = faiss.IndexLSH(dimension, nbits)\n",
    "# then add the data\n",
    "index.add(embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100, 131, 908, 910,  72, 125, 127, 128, 221, 337]], dtype=int64)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query\n",
    "xq0 = embeddings_array[100].reshape(1, dimension)\n",
    "\n",
    "# we use the search method to find the k nearest vectors\n",
    "D, I = index.search(xq0, k=10)\n",
    "# the indexes of these vectors are returned to I\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            paraphrase_all    cosine\n",
      "0  What made her so angry?  1.000000\n",
      "1     What made her angry?  0.977099\n",
      "7       What made her mad?  0.877078\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the corresponding text from X_train_emb\n",
    "nearest_neighbor_text = X_train.iloc[I[0]]['paraphrase_all']\n",
    "cos = pd.DataFrame(cosine_similarity(embeddings_array[I[0]], xq0)) #.reset_index(drop=True)\n",
    "# Print the nearest neighbor's text\n",
    "neighbor_sim = pd.concat([nearest_neighbor_text.reset_index(drop=True), cos.reset_index(drop=True)], axis = 1)\n",
    "neighbor_sim.columns = ['paraphrase_all', 'cosine']\n",
    "print(neighbor_sim[neighbor_sim['cosine'] > 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8345\n"
     ]
    }
   ],
   "source": [
    "#print(len(X_train_raw[X_train_raw['materials'] == 'plastic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 3., 3., 3., 3., 4., 4., 4., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbits = 2 --> cos = 0.57014906\n",
      "nbits = 4 --> cos = 0.85651\n",
      "nbits = 8 --> cos = 0.89414626\n",
      "nbits = 16 --> cos = 0.7429882\n",
      "nbits = 17 --> cos = 0.85651\n",
      "nbits = 18 --> cos = 0.47409043\n",
      "nbits = 19 --> cos = 0.79195905\n",
      "nbits = 20 --> cos = 0.56115973\n",
      "nbits = 21 --> cos = 0.7776756\n",
      "nbits = 22 --> cos = 0.799452\n",
      "nbits = 23 --> cos = 0.7774391\n",
      "nbits = 24 --> cos = 0.8799375\n",
      "nbits = 32 --> cos = 0.7316012\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "for nbits in [2, 4, 8, 16, 17, 18, 19, 20, 21, 22, 23, 24, 32]:\n",
    "    index = faiss.IndexLSH(dimension, nbits)\n",
    "    index.add(embeddings_array)\n",
    "    D, I = index.search(xq0, k=k)\n",
    "    cos = cosine_similarity(embeddings_array[I[0]], xq0)\n",
    "    print(\"nbits = %d --> cos = %s\" % (nbits, np.mean(cos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11,  31,   1, ..., 100, 223,   8], dtype=uint8)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract index binary codes (represented as int)\n",
    "arr = faiss.vector_to_array(index.codes)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000,)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_raw\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X_train_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# now translate them into the binary vector format\n",
    "arr_bites = (((arr[:, None] & (1 << np.arange(nbits)))) > 0).astype(int)\n",
    "print(len(arr_bites[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".paraphrasing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
